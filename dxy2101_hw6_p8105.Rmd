---
title: "dxy2101_hw6_p8105"
author: "Daisy Yan"
date: "2022-12-03"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(ggplot2)

set.seed(1)
```

## Problem 2

Load and prepare homicide data.

```{r homicides, message=FALSE, warning=FALSE}
# Load data
url = 'https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv'

homicide_data = read_csv(url)

# Create new variables and clean
homicide_data =
  homicide_data %>%
  mutate(city_state = str_c(city, state, sep = ", "),
         status = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved"
         )) %>%
  select(-city, -state, -disposition) %>%
  relocate(city_state, .before = lat) %>%
  subset(city_state != "Dallas, TX" & city_state != "Phoenix, AZ"
         & city_state != "Kansas City, MO" & city_state != "Tulsa, AL") %>%
  filter(victim_race == "White" | victim_race == "Black") %>%
  mutate(victim_age = as.numeric(victim_age))
```

For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors.

```{r logistic}
# Filter for Baltimore, MD
baltimore_df =
  homicide_data %>%
  filter(city_state == "Baltimore, MD") %>%
  mutate(
    resolved = as.numeric(status == "solved"),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)

# Fit logistic regression
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
  broom::tidy(conf.int = TRUE)

# Obtain estimate and CI of OR
fit_logistic %>% 
  mutate(
    OR = exp(estimate),
    low.lim = exp(conf.low),
    up.lim = exp(conf.high)) %>%
  select(term, OR, low.lim, up.lim) %>% 
  knitr::kable(digits = 3)
```

The odds ratio for solving homicides comparing male victims to female victims, controlling for age and race, is 0.426 (95% CI: 0.324, 0.558).

Now, run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims.

```{r glm_all, message=FALSE, warning=FALSE}
glm_cities = 
  homicide_data %>%
  mutate(
    resolved = as.numeric(status == "solved"),
    victim_race = fct_relevel(victim_race, "White")) %>%
  nest(data = -city_state) %>% 
  mutate(
    fit_logistic = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
    output = map(.x = fit_logistic, ~broom::tidy(.x,conf.int = TRUE))) %>% 
  select(city_state, output) %>% 
  unnest(output) %>% 
  mutate(
    OR = exp(estimate),
    low.lim = exp(conf.low),
    up.lim = exp(conf.high)) %>%
  select(city_state, term, OR, low.lim, up.lim) %>%
  filter(term == "victim_sexMale")

glm_cities %>%
  knitr::kable(digits = 3)
```

Create a plot that shows the estimated ORs and CIs for each city.

```{r plot, message=FALSE, warning=FALSE}
glm_cities %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + geom_point() + 
  geom_errorbar(aes(ymin = low.lim, ymax = up.lim), width = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("City, State") + ylab("OR of Solved Homicides") +
  ggtitle("Odds ratio and 95% CI of Solved Homicides by City, State")
```

According to the plot, New York, NY has the lowest odds ratio for solving homicides comparing male victims to female victims, controlling for age and race, while Albuquerque, NM has the highest odds ratio for solving homicides comparing male victims to female victims, controlling for age and race. Albuquerque, NM also has the greatest 95% confidence interval for the odds ratio, suggesting greater variability in its data. 

## Problem 3

Load and prepare birthweight data.

```{r birthweight, message=FALSE, warning=FALSE}
# Load data
birth_data = 
  read_csv("./data/birthweight.csv") %>%
  janitor::clean_names()

# Clean data
# make categorical variables factors
birth_data_tidy =
  birth_data %>%
  mutate(
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  )

# Check for missing values
which(is.na(birth_data_tidy))
```

There are no missing data points.

Propose a regression model for birthweight.

Because socioeconomic status has been shown to be an indicators for people's overall health and subsequently maternal health and child health, I will use the following variables as predictors. Physical variables were also included:

* family monthly income
* gestational age in weeks
* father’s race
* mother’s race
* mother’s pre-pregnancy weight

```{r model, message=FALSE, warning=FALSE}
# Fit regression model
lin_model = lm(bwt ~ fincome + gaweeks + frace + mrace + ppwt, data = birth_data_tidy) 

# Plot model residuals and fitted values
birth_data_tidy %>% 
  modelr::add_residuals(lin_model) %>%
  modelr::add_predictions(lin_model) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() +
  xlab("Fitted Values") + ylab("Residuals") +
  ggtitle("Modeled Residuals vs. Fitted Values")
```

Compare the proposed model to two others:

* One using length at birth and gestational age as predictors (main effects only)
* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r cv, message=FALSE, warning=FALSE}
# Fit model for main effects only
main = lm(bwt ~ blength + gaweeks, data = birth_data_tidy) 

# Fit model with interactions
inter = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex , data = birth_data_tidy) 

# Compare using cross-validated prediction error
cv_df =
  crossv_mc(birth_data_tidy, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    proposed = map(train, ~lm(bwt ~ fincome + gaweeks + frace + mrace + ppwt, data = .x)),
    main = map( train, ~lm(bwt  ~ blength + gaweeks, data = .x)),
    inter = map(train, ~lm(bwt  ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex , data = .x))) %>%
  mutate(
    rmse_proposed = map2_dbl(proposed, test, ~rmse(model = .x, data = .y)),
    rmse_main = map2_dbl(main, test, ~rmse(model = .x, data = .y)),
    rmse_inter = map2_dbl(inter, test, ~rmse(model = .x, data = .y)))

# Plot prediction error distribution for each candidate model
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
According to the plot, the model with interaction terms had the lowest RMSE, making it the model with the greatest predictive accuracy compared to the proposed model and main effect model. The proposed model had the highest RMSE. Therefore, I would choose the model with interaction terms. However, it should be noted that while it is the "best" model, it still has a high RMSE. Other models should also be explored.
